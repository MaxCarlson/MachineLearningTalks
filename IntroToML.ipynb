{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "### Machine Learning Paradigms\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Reinforcement learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "- **A Model**\n",
    "    - Predict something given some data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Inputs  \n",
    "\n",
    "- Data you want to learn a representation of\n",
    "- The input data can be viewed as a spreadsheet\n",
    "    - Each row is a single input to our model\n",
    "    - An input is composed of a set of features (columns in the spreadsheet)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Targets\n",
    "- What are we trying to predict?\n",
    "- A target represents the ideal output of a model for a given input\n",
    "- Commonly stored as the last column\n",
    "\n",
    "<br/> \n",
    "<br/> \n",
    "\n",
    "| Height | Weight | Gender | Age | Smokes | % chance of developing heart disease over next yr |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 71 | 165 | 1 | 27 | 0 | 0.053 |\n",
    "| 68 | 137 | 0 | 41 | 1 | 0.170 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnistDataset = datasets.MNIST('./data', train=True, download=True, \n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(\n",
    "                                        (0.1307,), (0.3081,))\n",
    "                                ]))\n",
    "\n",
    "mnistExample = DataLoader(mnistDataset, batch_size=1)\n",
    "batchIdx, (exampleInput, exampleTarget) = next(enumerate(mnistExample))\n",
    "x = exampleInput.detach().numpy().squeeze()\n",
    "plt.imshow(x, cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### A, b, c, d... \n",
    "\n",
    "- $x$ collection of input data, a vector of vectors \n",
    "    - $x_i$ individual sample of $x$\n",
    "        - a vector whose size is equal to the number of features being used\n",
    "- $n$ number of samples\n",
    "- $f$ model (function)\n",
    "- $y$ collection of targets/labels, the desired output for each sample in $x$\n",
    "    - Ground truth label  \n",
    "- $\\hat{y}$ output of our model (y-hat)  \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Representations\n",
    "\n",
    "- The goal of our model is to learn a representation of $ f(x) = y $\n",
    "- The actual output of our model: $f(x) = \\hat{y}$\n",
    "- The ideal:\n",
    "    - For each individual vector $x_i$ give us an output $\\hat{y_i}$ that is as close to the ground truth $y_i$ as possible\n",
    "    - $\\forall{x_i} \\in \\: x,\\space f(x_i) = \\hat{y_i} = y_i$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Neural Network\n",
    "\n",
    "- For a fully connected feed forward nueral network...\n",
    "    - Composed of layers of neurons\n",
    "    - Adjacent layer neurons always connected\n",
    "    - output size depends on target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/A-fully-connected-neural-network-with-two-hidden-layers.png\" width=\"400\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Layers\n",
    "\n",
    "- Composed of a number of neurons\n",
    "- For every connection, there is a weight $w$\n",
    "- $w^k_{ij} \\in \\mathbb{R}$\n",
    "    - $i$ input index\n",
    "    - $j$ neuron index\n",
    "    - $k$ layer index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Neurons\n",
    "\n",
    "<img src=\"images/neuron.png\" width=\"400\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neurons2\n",
    "\n",
    "- Layers of a network are composed of neurons\n",
    "- In a fully connected network, each neuron from $layer_k$ gives its output to each neuron in $layer_{k+1}$\n",
    "- A neuron has $n$ number of inputs\n",
    "- Activation function $\\phi$\n",
    "- For $neuron_j$\n",
    "    - Each input $x_i$ has an associated weight $w_{ij} \\in \\mathbb{R}$\n",
    "    - Net input $z_j = net_j = \\sum_{i=1}^{n}x_i w_{ij}$\n",
    "    - The output $\\phi(z_j) = o_j$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "- Simulate the on/off firing of a biological neuron\n",
    "- Must have a derivative at all points*\n",
    "- Linear\n",
    "    - $\\phi(z) = I(z) \\equiv mz + b$\n",
    "- Sigmoid\n",
    "    - $\\phi(z) = \\frac{1}{1 + e^{-z}}$\n",
    "    - $\\phi'(z) = \\phi(z)(1-\\phi(z))$\n",
    "- ReLU\n",
    "    - $\\phi(z) = maz(0, z)$\n",
    "    - $\\phi'(z) = z > 0 \\ ? \\ 1 : 0$\n",
    "    - *not differentiable at zero so derivative @ 0 is usually set to zero or 1\n",
    "- A number of other types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sigmoid\n",
    "<img src=\"images/sigmoid.png\" width=\"400\">\n",
    "\n",
    "#### ReLU\n",
    "<img src=\"images/relu.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (l2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (l3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "batchSize = 32\n",
    "train_loader = DataLoader(mnistDataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(mnistDataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example Output from batch training\n",
    "```\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322383\n",
    "Train Epoch: 1 [320/60000 (1%)]\tLoss: 0.620306\n",
    "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.545252\n",
    "Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.548669\n",
    "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.768465\n",
    "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.450810\n",
    "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.676172\n",
    "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.719184\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def saveNetwork(network, optimizer, modelName, epoch):\n",
    "    if not os.path.exists(f'results/{modelName}'):\n",
    "        os.makedirs(f'results/{modelName}')\n",
    "    torch.save(network.state_dict(), f'results/{modelName}/{epoch}.pth')\n",
    "    torch.save(optimizer.state_dict(), f'results/{modelName}/{epoch}.pth')\n",
    "\n",
    "log_interval = 10\n",
    "def batchWork(network, optimizer, modelName, batch_idx, epoch, loss, train_losses, train_counter, printBatchInfo):   \n",
    "    if printBatchInfo:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_losses.append(loss.item())\n",
    "    train_counter.append(\n",
    "        (batch_idx*batchSize) + ((epoch-1)*len(train_loader.dataset)))\n",
    "    saveNetwork(network, optimizer, modelName, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.493481\n",
      "Train Epoch: 1 [280/60000 (1%)]\tLoss: 1.003825\n",
      "Train Epoch: 1 [560/60000 (1%)]\tLoss: 0.354426\n",
      "Train Epoch: 1 [840/60000 (2%)]\tLoss: 0.406406\n",
      "Train Epoch: 1 [1120/60000 (2%)]\tLoss: 0.628384\n",
      "Train Epoch: 1 [1400/60000 (3%)]\tLoss: 0.353571\n",
      "Train Epoch: 1 [1680/60000 (3%)]\tLoss: 0.982613\n",
      "Train Epoch: 1 [1960/60000 (4%)]\tLoss: 0.235507\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.711687\n",
      "Train Epoch: 1 [2520/60000 (5%)]\tLoss: 0.491538\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 0.248385\n",
      "Train Epoch: 1 [3080/60000 (6%)]\tLoss: 0.371612\n",
      "Train Epoch: 1 [3360/60000 (6%)]\tLoss: 0.381937\n",
      "Train Epoch: 1 [3640/60000 (7%)]\tLoss: 0.447612\n",
      "Train Epoch: 1 [3920/60000 (7%)]\tLoss: 0.273762\n",
      "Train Epoch: 1 [4200/60000 (8%)]\tLoss: 0.493879\n",
      "Train Epoch: 1 [4480/60000 (9%)]\tLoss: 0.731136\n",
      "Train Epoch: 1 [4760/60000 (9%)]\tLoss: 0.463227\n",
      "Train Epoch: 1 [5040/60000 (10%)]\tLoss: 0.588381\n",
      "Train Epoch: 1 [5320/60000 (10%)]\tLoss: 0.694965\n",
      "Train Epoch: 1 [5600/60000 (11%)]\tLoss: 0.399225\n",
      "Train Epoch: 1 [5880/60000 (11%)]\tLoss: 0.769826\n",
      "Train Epoch: 1 [6160/60000 (12%)]\tLoss: 0.761788\n",
      "Train Epoch: 1 [6440/60000 (12%)]\tLoss: 0.201964\n",
      "Train Epoch: 1 [6720/60000 (13%)]\tLoss: 0.157022\n",
      "Train Epoch: 1 [7000/60000 (13%)]\tLoss: 0.264936\n",
      "Train Epoch: 1 [7280/60000 (14%)]\tLoss: 0.534916\n",
      "Train Epoch: 1 [7560/60000 (14%)]\tLoss: 1.299793\n",
      "Train Epoch: 1 [7840/60000 (15%)]\tLoss: 0.336432\n",
      "Train Epoch: 1 [8120/60000 (15%)]\tLoss: 0.281296\n",
      "Train Epoch: 1 [8400/60000 (16%)]\tLoss: 0.355418\n",
      "Train Epoch: 1 [8680/60000 (17%)]\tLoss: 0.065430\n",
      "Train Epoch: 1 [8960/60000 (17%)]\tLoss: 0.294986\n",
      "Train Epoch: 1 [9240/60000 (18%)]\tLoss: 0.337269\n",
      "Train Epoch: 1 [9520/60000 (18%)]\tLoss: 0.494274\n",
      "Train Epoch: 1 [9800/60000 (19%)]\tLoss: 0.173206\n",
      "Train Epoch: 1 [10080/60000 (19%)]\tLoss: 0.239853\n",
      "Train Epoch: 1 [10360/60000 (20%)]\tLoss: 0.181200\n",
      "Train Epoch: 1 [10640/60000 (20%)]\tLoss: 0.308231\n",
      "Train Epoch: 1 [10920/60000 (21%)]\tLoss: 0.189882\n",
      "Train Epoch: 1 [11200/60000 (21%)]\tLoss: 0.259567\n",
      "Train Epoch: 1 [11480/60000 (22%)]\tLoss: 0.341116\n",
      "Train Epoch: 1 [11760/60000 (22%)]\tLoss: 0.213679\n",
      "Train Epoch: 1 [12040/60000 (23%)]\tLoss: 0.259946\n",
      "Train Epoch: 1 [12320/60000 (23%)]\tLoss: 0.430157\n",
      "Train Epoch: 1 [12600/60000 (24%)]\tLoss: 0.288117\n",
      "Train Epoch: 1 [12880/60000 (25%)]\tLoss: 0.064522\n",
      "Train Epoch: 1 [13160/60000 (25%)]\tLoss: 0.132504\n",
      "Train Epoch: 1 [13440/60000 (26%)]\tLoss: 0.366813\n",
      "Train Epoch: 1 [13720/60000 (26%)]\tLoss: 0.606775\n",
      "Train Epoch: 1 [14000/60000 (27%)]\tLoss: 0.258829\n",
      "Train Epoch: 1 [14280/60000 (27%)]\tLoss: 0.293339\n",
      "Train Epoch: 1 [14560/60000 (28%)]\tLoss: 0.375804\n",
      "Train Epoch: 1 [14840/60000 (28%)]\tLoss: 0.245912\n",
      "Train Epoch: 1 [15120/60000 (29%)]\tLoss: 0.185308\n",
      "Train Epoch: 1 [15400/60000 (29%)]\tLoss: 0.187016\n",
      "Train Epoch: 1 [15680/60000 (30%)]\tLoss: 0.361795\n",
      "Train Epoch: 1 [15960/60000 (30%)]\tLoss: 0.538536\n",
      "Train Epoch: 1 [16240/60000 (31%)]\tLoss: 0.122706\n",
      "Train Epoch: 1 [16520/60000 (31%)]\tLoss: 0.480184\n",
      "Train Epoch: 1 [16800/60000 (32%)]\tLoss: 0.227853\n",
      "Train Epoch: 1 [17080/60000 (33%)]\tLoss: 0.310109\n",
      "Train Epoch: 1 [17360/60000 (33%)]\tLoss: 0.101860\n",
      "Train Epoch: 1 [17640/60000 (34%)]\tLoss: 0.293808\n",
      "Train Epoch: 1 [17920/60000 (34%)]\tLoss: 0.369112\n",
      "Train Epoch: 1 [18200/60000 (35%)]\tLoss: 0.340292\n",
      "Train Epoch: 1 [18480/60000 (35%)]\tLoss: 0.285187\n",
      "Train Epoch: 1 [18760/60000 (36%)]\tLoss: 0.190858\n",
      "Train Epoch: 1 [19040/60000 (36%)]\tLoss: 0.649171\n",
      "Train Epoch: 1 [19320/60000 (37%)]\tLoss: 0.457087\n",
      "Train Epoch: 1 [19600/60000 (37%)]\tLoss: 0.089566\n",
      "Train Epoch: 1 [19880/60000 (38%)]\tLoss: 0.750684\n",
      "Train Epoch: 1 [20160/60000 (38%)]\tLoss: 0.467508\n",
      "Train Epoch: 1 [20440/60000 (39%)]\tLoss: 0.225447\n",
      "Train Epoch: 1 [20720/60000 (39%)]\tLoss: 0.308496\n",
      "Train Epoch: 1 [21000/60000 (40%)]\tLoss: 0.785052\n",
      "Train Epoch: 1 [21280/60000 (41%)]\tLoss: 0.243224\n",
      "Train Epoch: 1 [21560/60000 (41%)]\tLoss: 0.286446\n",
      "Train Epoch: 1 [21840/60000 (42%)]\tLoss: 0.502322\n",
      "Train Epoch: 1 [22120/60000 (42%)]\tLoss: 1.062000\n",
      "Train Epoch: 1 [22400/60000 (43%)]\tLoss: 0.321465\n",
      "Train Epoch: 1 [22680/60000 (43%)]\tLoss: 0.858635\n",
      "Train Epoch: 1 [22960/60000 (44%)]\tLoss: 0.537172\n",
      "Train Epoch: 1 [23240/60000 (44%)]\tLoss: 0.159594\n",
      "Train Epoch: 1 [23520/60000 (45%)]\tLoss: 0.350424\n",
      "Train Epoch: 1 [23800/60000 (45%)]\tLoss: 0.226255\n",
      "Train Epoch: 1 [24080/60000 (46%)]\tLoss: 0.905297\n",
      "Train Epoch: 1 [24360/60000 (46%)]\tLoss: 0.192700\n",
      "Train Epoch: 1 [24640/60000 (47%)]\tLoss: 0.540922\n",
      "Train Epoch: 1 [24920/60000 (47%)]\tLoss: 0.343441\n",
      "Train Epoch: 1 [25200/60000 (48%)]\tLoss: 0.401391\n",
      "Train Epoch: 1 [25480/60000 (49%)]\tLoss: 0.141126\n",
      "Train Epoch: 1 [25760/60000 (49%)]\tLoss: 0.377868\n",
      "Train Epoch: 1 [26040/60000 (50%)]\tLoss: 0.236532\n",
      "Train Epoch: 1 [26320/60000 (50%)]\tLoss: 0.264189\n",
      "Train Epoch: 1 [26600/60000 (51%)]\tLoss: 0.336279\n",
      "Train Epoch: 1 [26880/60000 (51%)]\tLoss: 0.103108\n",
      "Train Epoch: 1 [27160/60000 (52%)]\tLoss: 0.050794\n",
      "Train Epoch: 1 [27440/60000 (52%)]\tLoss: 0.058473\n",
      "Train Epoch: 1 [27720/60000 (53%)]\tLoss: 0.195749\n",
      "Train Epoch: 1 [28000/60000 (53%)]\tLoss: 0.174926\n",
      "Train Epoch: 1 [28280/60000 (54%)]\tLoss: 0.130981\n",
      "Train Epoch: 1 [28560/60000 (54%)]\tLoss: 0.233927\n",
      "Train Epoch: 1 [28840/60000 (55%)]\tLoss: 0.228811\n",
      "Train Epoch: 1 [29120/60000 (55%)]\tLoss: 0.220584\n",
      "Train Epoch: 1 [29400/60000 (56%)]\tLoss: 0.195012\n",
      "Train Epoch: 1 [29680/60000 (57%)]\tLoss: 0.422430\n",
      "Train Epoch: 1 [29960/60000 (57%)]\tLoss: 0.147391\n",
      "Train Epoch: 1 [30240/60000 (58%)]\tLoss: 0.079704\n",
      "Train Epoch: 1 [30520/60000 (58%)]\tLoss: 0.409108\n",
      "Train Epoch: 1 [30800/60000 (59%)]\tLoss: 0.570537\n",
      "Train Epoch: 1 [31080/60000 (59%)]\tLoss: 0.174072\n",
      "Train Epoch: 1 [31360/60000 (60%)]\tLoss: 0.778611\n",
      "Train Epoch: 1 [31640/60000 (60%)]\tLoss: 0.723080\n",
      "Train Epoch: 1 [31920/60000 (61%)]\tLoss: 0.193041\n",
      "Train Epoch: 1 [32200/60000 (61%)]\tLoss: 0.239004\n",
      "Train Epoch: 1 [32480/60000 (62%)]\tLoss: 0.502682\n",
      "Train Epoch: 1 [32760/60000 (62%)]\tLoss: 0.379695\n",
      "Train Epoch: 1 [33040/60000 (63%)]\tLoss: 0.341786\n",
      "Train Epoch: 1 [33320/60000 (63%)]\tLoss: 0.361896\n",
      "Train Epoch: 1 [33600/60000 (64%)]\tLoss: 0.354059\n",
      "Train Epoch: 1 [33880/60000 (65%)]\tLoss: 0.329270\n",
      "Train Epoch: 1 [34160/60000 (65%)]\tLoss: 0.120022\n",
      "Train Epoch: 1 [34440/60000 (66%)]\tLoss: 0.288176\n",
      "Train Epoch: 1 [34720/60000 (66%)]\tLoss: 0.091796\n",
      "Train Epoch: 1 [35000/60000 (67%)]\tLoss: 0.034504\n",
      "Train Epoch: 1 [35280/60000 (67%)]\tLoss: 0.280197\n",
      "Train Epoch: 1 [35560/60000 (68%)]\tLoss: 0.415760\n",
      "Train Epoch: 1 [35840/60000 (68%)]\tLoss: 0.346707\n",
      "Train Epoch: 1 [36120/60000 (69%)]\tLoss: 0.170184\n",
      "Train Epoch: 1 [36400/60000 (69%)]\tLoss: 0.046396\n",
      "Train Epoch: 1 [36680/60000 (70%)]\tLoss: 0.333956\n",
      "Train Epoch: 1 [36960/60000 (70%)]\tLoss: 0.400714\n",
      "Train Epoch: 1 [37240/60000 (71%)]\tLoss: 0.060764\n",
      "Train Epoch: 1 [37520/60000 (71%)]\tLoss: 0.330839\n",
      "Train Epoch: 1 [37800/60000 (72%)]\tLoss: 0.460848\n",
      "Train Epoch: 1 [38080/60000 (73%)]\tLoss: 0.234481\n",
      "Train Epoch: 1 [38360/60000 (73%)]\tLoss: 0.044239\n",
      "Train Epoch: 1 [38640/60000 (74%)]\tLoss: 0.374321\n",
      "Train Epoch: 1 [38920/60000 (74%)]\tLoss: 0.106339\n",
      "Train Epoch: 1 [39200/60000 (75%)]\tLoss: 1.071089\n",
      "Train Epoch: 1 [39480/60000 (75%)]\tLoss: 0.256124\n",
      "Train Epoch: 1 [39760/60000 (76%)]\tLoss: 0.057616\n",
      "Train Epoch: 1 [40040/60000 (76%)]\tLoss: 0.067253\n",
      "Train Epoch: 1 [40320/60000 (77%)]\tLoss: 0.567109\n",
      "Train Epoch: 1 [40600/60000 (77%)]\tLoss: 0.676700\n",
      "Train Epoch: 1 [40880/60000 (78%)]\tLoss: 0.222293\n",
      "Train Epoch: 1 [41160/60000 (78%)]\tLoss: 0.016963\n",
      "Train Epoch: 1 [41440/60000 (79%)]\tLoss: 0.335008\n",
      "Train Epoch: 1 [41720/60000 (79%)]\tLoss: 0.108177\n",
      "Train Epoch: 1 [42000/60000 (80%)]\tLoss: 0.867844\n",
      "Train Epoch: 1 [42280/60000 (81%)]\tLoss: 0.431237\n",
      "Train Epoch: 1 [42560/60000 (81%)]\tLoss: 0.587357\n",
      "Train Epoch: 1 [42840/60000 (82%)]\tLoss: 0.107156\n",
      "Train Epoch: 1 [43120/60000 (82%)]\tLoss: 0.234612\n",
      "Train Epoch: 1 [43400/60000 (83%)]\tLoss: 0.118894\n",
      "Train Epoch: 1 [43680/60000 (83%)]\tLoss: 0.022644\n",
      "Train Epoch: 1 [43960/60000 (84%)]\tLoss: 0.309128\n",
      "Train Epoch: 1 [44240/60000 (84%)]\tLoss: 0.335879\n",
      "Train Epoch: 1 [44520/60000 (85%)]\tLoss: 0.231894\n",
      "Train Epoch: 1 [44800/60000 (85%)]\tLoss: 0.124311\n",
      "Train Epoch: 1 [45080/60000 (86%)]\tLoss: 0.281174\n",
      "Train Epoch: 1 [45360/60000 (86%)]\tLoss: 0.301682\n",
      "Train Epoch: 1 [45640/60000 (87%)]\tLoss: 0.877899\n",
      "Train Epoch: 1 [45920/60000 (87%)]\tLoss: 0.095593\n",
      "Train Epoch: 1 [46200/60000 (88%)]\tLoss: 0.207765\n",
      "Train Epoch: 1 [46480/60000 (89%)]\tLoss: 0.383953\n",
      "Train Epoch: 1 [46760/60000 (89%)]\tLoss: 0.025189\n",
      "Train Epoch: 1 [47040/60000 (90%)]\tLoss: 0.208512\n",
      "Train Epoch: 1 [47320/60000 (90%)]\tLoss: 0.393539\n",
      "Train Epoch: 1 [47600/60000 (91%)]\tLoss: 0.148626\n",
      "Train Epoch: 1 [47880/60000 (91%)]\tLoss: 0.230787\n",
      "Train Epoch: 1 [48160/60000 (92%)]\tLoss: 0.250572\n",
      "Train Epoch: 1 [48440/60000 (92%)]\tLoss: 0.086978\n",
      "Train Epoch: 1 [48720/60000 (93%)]\tLoss: 0.249461\n",
      "Train Epoch: 1 [49000/60000 (93%)]\tLoss: 0.126637\n",
      "Train Epoch: 1 [49280/60000 (94%)]\tLoss: 0.109327\n",
      "Train Epoch: 1 [49560/60000 (94%)]\tLoss: 0.290378\n",
      "Train Epoch: 1 [49840/60000 (95%)]\tLoss: 0.660340\n",
      "Train Epoch: 1 [50120/60000 (95%)]\tLoss: 0.565336\n",
      "Train Epoch: 1 [50400/60000 (96%)]\tLoss: 0.252810\n",
      "Train Epoch: 1 [50680/60000 (97%)]\tLoss: 0.470440\n",
      "Train Epoch: 1 [50960/60000 (97%)]\tLoss: 0.412422\n",
      "Train Epoch: 1 [51240/60000 (98%)]\tLoss: 0.019789\n",
      "Train Epoch: 1 [51520/60000 (98%)]\tLoss: 0.199346\n",
      "Train Epoch: 1 [51800/60000 (99%)]\tLoss: 0.039524\n",
      "Train Epoch: 1 [52080/60000 (99%)]\tLoss: 0.115450\n",
      "Train Epoch: 1 [52360/60000 (100%)]\tLoss: 0.122769\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]\n",
    "\n",
    "def train(epoch, modelName, printBatchInfo = False):\n",
    "    network.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = network(x)\n",
    "        loss = lossFn(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            batchWork(network, optimizer, modelName, batch_idx, \n",
    "                      epoch, loss, train_losses, train_counter, printBatchInfo)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch, modelName='tmpModel', printBatchInfo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            output = network(x)\n",
    "            test_loss += lossFn(output, y).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.data.view_as(pred)).sum() \n",
    "    \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "test()\n",
    "for epoch in range(1, epochs + 1):\n",
    "  train(epoch, modelName = 'fc_768_128_10', printBatchInfo=False)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Error\n",
    "\n",
    "- How inaccurate is our model?\n",
    "\n",
    "- $E = MAE = \\dfrac{|\\hat{y} - y|}{n} = \\dfrac{\\sum_{i}^{n}{|\\hat{y}_i - y_i|}}{n} = L1$\n",
    "\n",
    "- $E = MSE = \\dfrac{(\\hat{y} - y)^2}{n} = \\dfrac{\\sum_{i}^{n}{(\\hat{y}_i - y_i)^2}}{n} = L2$\n",
    "- Ex:\n",
    "    - $y = [1, 2, 3]$\n",
    "    - $\\hat{y} = [7, 2, -2]$\n",
    "    \n",
    "    - $MAE = \\dfrac{(7-1)+(2-2)+(-2-3)}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Loss\n",
    "\n",
    "- Your error & loss function are coupled \n",
    "- Your error function must be differentiable\n",
    "- $C = loss fn = (Cost function)$\n",
    "- $SqErr = e_i = (\\hat{y_i} - y_i)^2$  \n",
    "\n",
    "- $SEL = C = 2(\\hat{y} - y)$\n",
    "    - $(\\hat{y} - y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "1. Determine what the input data for the model will be\n",
    "2. Build training dataset\n",
    "    - inputs & target/label for each sample\n",
    "3. Determine what type of learning model to use & the structure of it\n",
    "    - NN, Random Forest, Gradient Boost, etc.\n",
    "4. Set hyperparameters & run the model over the training set\n",
    "    - Usually model will process the training set in batches of samples\n",
    "    - Walk the model down the optimization landscape\n",
    "5. Determine accuracy of model/function on the so far unused test-set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
