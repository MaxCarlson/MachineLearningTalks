{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "### Machine Learning Paradigms\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Reinforcement learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "- **A Model**\n",
    "    - Predict something given some data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Inputs  \n",
    "\n",
    "- Data you want to learn a representation of\n",
    "- The input data can be viewed as a spreadsheet\n",
    "    - Each row is a single input to our model\n",
    "    - An input is composed of a set of features (columns in the spreadsheet)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Targets\n",
    "- What are we trying to predict?\n",
    "- A target represents the ideal output of a model for a given input\n",
    "- Commonly stored as the last column\n",
    "\n",
    "<br/> \n",
    "<br/> \n",
    "\n",
    "| Height | Weight | Gender | Age | Smokes | % chance of developing heart disease over next yr |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| 71 | 165 | 1 | 27 | 0 | 0.053 |\n",
    "| 68 | 137 | 0 | 41 | 1 | 0.170 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mnistDataset = datasets.MNIST('./data', train=True, download=True, \n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(\n",
    "                                        (0.1307,), (0.3081,))\n",
    "                                ]))\n",
    "\n",
    "mnistExample = DataLoader(mnistDataset, batch_size=1)\n",
    "batchIdx, (exampleInput, exampleTarget) = next(enumerate(mnistExample))\n",
    "x = exampleInput.detach().numpy().squeeze()\n",
    "plt.imshow(x, cmap='gray')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### A, b, c, d... \n",
    "\n",
    "- $x$ collection of input data, a vector of vectors \n",
    "    - $x_i$ individual sample of $x$\n",
    "        - a vector whose size is equal to the number of features being used\n",
    "- $n$ number of samples\n",
    "- $f$ model (function)\n",
    "- $y$ collection of targets/labels, the desired output for each sample in $x$\n",
    "    - Ground truth label  \n",
    "- $\\hat{y}$ output of our model (y-hat)  \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Representations\n",
    "\n",
    "- The goal of our model is to learn a representation of $ f(x) = y $\n",
    "- The actual output of our model: $f(x) = \\hat{y}$\n",
    "- The ideal:\n",
    "    - For each individual vector $x_i$ give us an output $\\hat{y_i}$ that is as close to the ground truth $y_i$ as possible\n",
    "    - $\\forall{x_i} \\in \\: x,\\space f(x_i) = \\hat{y_i} = y_i$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Neural Network\n",
    "\n",
    "- For a fully connected feed forward nueral network...\n",
    "    - Composed of layers of neurons\n",
    "    - Adjacent layer neurons always connected\n",
    "    - output size depends on target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/A-fully-connected-neural-network-with-two-hidden-layers.png\" width=\"400\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Layers\n",
    "\n",
    "- Composed of a number of neurons\n",
    "- For every connection, there is a weight $w$\n",
    "- $w^k_{ij} \\in \\mathbb{R}$\n",
    "    - $i$ input index\n",
    "    - $j$ neuron index\n",
    "    - $k$ layer index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Neurons\n",
    "\n",
    "<img src=\"images/neuron.png\" width=\"400\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neurons2\n",
    "\n",
    "- Layers of a network are composed of neurons\n",
    "- In a fully connected network, each neuron from $layer_k$ gives its output to each neuron in $layer_{k+1}$\n",
    "- A neuron has $n$ number of inputs\n",
    "- Activation function $\\phi$\n",
    "- For $neuron_j$\n",
    "    - Each input $x_i$ has an associated weight $w_{ij} \\in \\mathbb{R}$\n",
    "    - Net input $z_j = net_j = \\sum_{i=1}^{n}x_i w_{ij}$\n",
    "    - The output $\\phi(z_j) = o_j$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "- Simulate the on/off firing of a biological neuron\n",
    "- Must have a derivative at all points*\n",
    "- Linear\n",
    "    - $\\phi(z) = I(z) \\equiv mz + b$\n",
    "- Sigmoid\n",
    "    - $\\phi(z) = \\frac{1}{1 + e^{-z}}$\n",
    "    - $\\phi'(z) = \\phi(z)(1-\\phi(z))$\n",
    "- ReLU\n",
    "    - $\\phi(z) = maz(0, z)$\n",
    "    - $\\phi'(z) = z > 0 \\ ? \\ 1 : 0$\n",
    "    - *not differentiable at zero so derivative @ 0 is usually set to zero or 1\n",
    "- A number of other types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sigmoid\n",
    "<img src=\"images/sigmoid.png\" width=\"400\">\n",
    "\n",
    "#### ReLU\n",
    "<img src=\"images/relu.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (l1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (l2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (l3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "batchSize = 32\n",
    "train_loader = DataLoader(mnistDataset, batch_size=batchSize, shuffle=True)\n",
    "test_loader = DataLoader(mnistDataset, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.Linear(784, 128)\n",
    "        self.l2 = nn.Linear(128, 128)\n",
    "        self.l3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example Output from batch training\n",
    "```\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322383\n",
    "Train Epoch: 1 [320/60000 (1%)]\tLoss: 0.620306\n",
    "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.545252\n",
    "Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.548669\n",
    "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.768465\n",
    "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.450810\n",
    "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.676172\n",
    "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.719184\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.335976\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: 0.019091\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.234903\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.271222\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.159377\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.197825\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.497088\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.492190\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.018640\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 0.114251\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.293569\n",
      "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 0.273756\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.564557\n",
      "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 0.498226\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.051699\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.021935\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.003907\n",
      "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 0.685615\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.362373\n",
      "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 0.334253\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.213016\n",
      "Train Epoch: 1 [6720/60000 (11%)]\tLoss: 0.057238\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.434545\n",
      "Train Epoch: 1 [7360/60000 (12%)]\tLoss: 0.128183\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.300092\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.234135\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.009785\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 0.139973\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.181357\n",
      "Train Epoch: 1 [9280/60000 (15%)]\tLoss: 0.166626\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.085444\n",
      "Train Epoch: 1 [9920/60000 (17%)]\tLoss: 0.285852\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.406168\n",
      "Train Epoch: 1 [10560/60000 (18%)]\tLoss: 0.085501\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.048436\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.425974\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.184568\n",
      "Train Epoch: 1 [11840/60000 (20%)]\tLoss: 0.323770\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.003687\n",
      "Train Epoch: 1 [12480/60000 (21%)]\tLoss: 0.011282\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.002695\n",
      "Train Epoch: 1 [13120/60000 (22%)]\tLoss: 0.093486\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.188132\n",
      "Train Epoch: 1 [13760/60000 (23%)]\tLoss: 0.019690\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.097743\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.277326\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.106900\n",
      "Train Epoch: 1 [15040/60000 (25%)]\tLoss: 0.015969\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.316484\n",
      "Train Epoch: 1 [15680/60000 (26%)]\tLoss: 0.395976\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.399031\n",
      "Train Epoch: 1 [16320/60000 (27%)]\tLoss: 0.149021\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.362669\n",
      "Train Epoch: 1 [16960/60000 (28%)]\tLoss: 0.200305\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.161497\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.028173\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.494196\n",
      "Train Epoch: 1 [18240/60000 (30%)]\tLoss: 0.354998\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.057450\n",
      "Train Epoch: 1 [18880/60000 (31%)]\tLoss: 0.925555\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.270034\n",
      "Train Epoch: 1 [19520/60000 (33%)]\tLoss: 0.138647\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.023075\n",
      "Train Epoch: 1 [20160/60000 (34%)]\tLoss: 0.036316\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.353898\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.735298\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.116016\n",
      "Train Epoch: 1 [21440/60000 (36%)]\tLoss: 0.033958\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.015584\n",
      "Train Epoch: 1 [22080/60000 (37%)]\tLoss: 0.469097\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.074370\n",
      "Train Epoch: 1 [22720/60000 (38%)]\tLoss: 0.010821\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.473818\n",
      "Train Epoch: 1 [23360/60000 (39%)]\tLoss: 0.024127\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.001218\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.125075\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.156294\n",
      "Train Epoch: 1 [24640/60000 (41%)]\tLoss: 0.077480\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.056037\n",
      "Train Epoch: 1 [25280/60000 (42%)]\tLoss: 0.179630\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.009293\n",
      "Train Epoch: 1 [25920/60000 (43%)]\tLoss: 0.232134\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.139191\n",
      "Train Epoch: 1 [26560/60000 (44%)]\tLoss: 0.241492\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.438135\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.893028\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.318730\n",
      "Train Epoch: 1 [27840/60000 (46%)]\tLoss: 0.365786\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.511178\n",
      "Train Epoch: 1 [28480/60000 (47%)]\tLoss: 0.556730\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.331512\n",
      "Train Epoch: 1 [29120/60000 (49%)]\tLoss: 0.320179\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.261198\n",
      "Train Epoch: 1 [29760/60000 (50%)]\tLoss: 0.318066\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.291051\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.078248\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.220260\n",
      "Train Epoch: 1 [31040/60000 (52%)]\tLoss: 0.068949\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.526774\n",
      "Train Epoch: 1 [31680/60000 (53%)]\tLoss: 0.418901\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.014031\n",
      "Train Epoch: 1 [32320/60000 (54%)]\tLoss: 0.493569\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.213986\n",
      "Train Epoch: 1 [32960/60000 (55%)]\tLoss: 0.043784\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.495954\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.240698\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.070829\n",
      "Train Epoch: 1 [34240/60000 (57%)]\tLoss: 0.166264\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.192797\n",
      "Train Epoch: 1 [34880/60000 (58%)]\tLoss: 0.006433\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.106257\n",
      "Train Epoch: 1 [35520/60000 (59%)]\tLoss: 0.121738\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.503216\n",
      "Train Epoch: 1 [36160/60000 (60%)]\tLoss: 0.085177\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.167714\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.067677\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.461844\n",
      "Train Epoch: 1 [37440/60000 (62%)]\tLoss: 0.083523\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.072971\n",
      "Train Epoch: 1 [38080/60000 (63%)]\tLoss: 1.057315\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.082900\n",
      "Train Epoch: 1 [38720/60000 (65%)]\tLoss: 0.516940\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.168329\n",
      "Train Epoch: 1 [39360/60000 (66%)]\tLoss: 0.293886\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.284111\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.007181\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.274582\n",
      "Train Epoch: 1 [40640/60000 (68%)]\tLoss: 0.341921\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.036221\n",
      "Train Epoch: 1 [41280/60000 (69%)]\tLoss: 0.301632\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.135859\n",
      "Train Epoch: 1 [41920/60000 (70%)]\tLoss: 0.194668\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.416219\n",
      "Train Epoch: 1 [42560/60000 (71%)]\tLoss: 0.070810\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.264619\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.019866\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 0.298452\n",
      "Train Epoch: 1 [43840/60000 (73%)]\tLoss: 0.588715\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.059388\n",
      "Train Epoch: 1 [44480/60000 (74%)]\tLoss: 0.004744\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.030237\n",
      "Train Epoch: 1 [45120/60000 (75%)]\tLoss: 0.491233\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.065679\n",
      "Train Epoch: 1 [45760/60000 (76%)]\tLoss: 0.636352\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.084930\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.191953\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.134629\n",
      "Train Epoch: 1 [47040/60000 (78%)]\tLoss: 0.203788\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.119022\n",
      "Train Epoch: 1 [47680/60000 (79%)]\tLoss: 0.409691\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.052811\n",
      "Train Epoch: 1 [48320/60000 (81%)]\tLoss: 0.266021\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.115105\n",
      "Train Epoch: 1 [48960/60000 (82%)]\tLoss: 0.185577\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.094992\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.145633\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.227198\n",
      "Train Epoch: 1 [50240/60000 (84%)]\tLoss: 0.032685\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.008850\n",
      "Train Epoch: 1 [50880/60000 (85%)]\tLoss: 0.003013\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.139852\n",
      "Train Epoch: 1 [51520/60000 (86%)]\tLoss: 0.068312\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.044108\n",
      "Train Epoch: 1 [52160/60000 (87%)]\tLoss: 0.266252\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.187866\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.008031\n",
      "Train Epoch: 1 [53120/60000 (89%)]\tLoss: 0.684231\n",
      "Train Epoch: 1 [53440/60000 (89%)]\tLoss: 0.091630\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.376793\n",
      "Train Epoch: 1 [54080/60000 (90%)]\tLoss: 0.569795\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.365679\n",
      "Train Epoch: 1 [54720/60000 (91%)]\tLoss: 0.148163\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.116573\n",
      "Train Epoch: 1 [55360/60000 (92%)]\tLoss: 0.003349\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.390463\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.032534\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.116740\n",
      "Train Epoch: 1 [56640/60000 (94%)]\tLoss: 0.241447\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.209578\n",
      "Train Epoch: 1 [57280/60000 (95%)]\tLoss: 0.080590\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.132966\n",
      "Train Epoch: 1 [57920/60000 (97%)]\tLoss: 0.004365\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.058975\n",
      "Train Epoch: 1 [58560/60000 (98%)]\tLoss: 0.145039\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.160294\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.129011\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.028784\n",
      "Train Epoch: 1 [59840/60000 (100%)]\tLoss: 0.100556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "log_interval = 10\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]\n",
    "\n",
    "def train(epoch, modelName, printBatchInfo = False):\n",
    "    network.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = network(x)\n",
    "\n",
    "        loss = lossFn(output, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            if printBatchInfo:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "                (batch_idx*batchSize) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), f'results/{modelName}.pth')\n",
    "            torch.save(optimizer.state_dict(), f'results/{modelName}.pth')\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch, modelName='tmpModel', printBatchInfo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0049, Accuracy: 57613/60000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0059, Accuracy: 57160/60000 (95%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0052, Accuracy: 57421/60000 (96%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: 0.0046, Accuracy: 57916/60000 (97%)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m test()\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 20\u001b[0m   train(epoch, modelName \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mfc_768_128_10\u001b[39;49m\u001b[39m'\u001b[39;49m, printBatchInfo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     21\u001b[0m   test()\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, modelName, printBatchInfo)\u001b[0m\n\u001b[1;32m     17\u001b[0m output \u001b[39m=\u001b[39m network(x)\n\u001b[1;32m     19\u001b[0m loss \u001b[39m=\u001b[39m lossFn(output, y)\n\u001b[0;32m---> 20\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m log_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            output = network(x)\n",
    "            test_loss += lossFn(output, y).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(y.data.view_as(pred)).sum() \n",
    "    \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "        print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "test()\n",
    "for epoch in range(1, epochs + 1):\n",
    "  train(epoch, modelName = 'fc_768_128_10', printBatchInfo=False)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Error\n",
    "\n",
    "- How inaccurate is our model?\n",
    "\n",
    "- $E = MAE = \\dfrac{|\\hat{y} - y|}{n} = \\dfrac{\\sum_{i}^{n}{|\\hat{y}_i - y_i|}}{n} = L1$\n",
    "\n",
    "- $E = MSE = \\dfrac{(\\hat{y} - y)^2}{n} = \\dfrac{\\sum_{i}^{n}{(\\hat{y}_i - y_i)^2}}{n} = L2$\n",
    "- Ex:\n",
    "    - $y = [1, 2, 3]$\n",
    "    - $\\hat{y} = [7, 2, -2]$\n",
    "    \n",
    "    - $MAE = \\dfrac{(7-1)+(2-2)+(-2-3)}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Loss\n",
    "\n",
    "- Your error & loss function are coupled \n",
    "- Your error function must be differentiable\n",
    "- $C = loss fn = (Cost function)$\n",
    "- $SqErr = e_i = (\\hat{y_i} - y_i)^2$  \n",
    "\n",
    "- $SEL = C = 2(\\hat{y} - y)$\n",
    "    - $(\\hat{y} - y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "1. Determine what the input data for the model will be\n",
    "2. Build training dataset\n",
    "    - inputs & target/label for each sample\n",
    "3. Determine what type of learning model to use & the structure of it\n",
    "    - NN, Random Forest, Gradient Boost, etc.\n",
    "4. Set hyperparameters & run the model over the training set\n",
    "    - Usually model will process the training set in batches of samples\n",
    "    - Walk the model down the optimization landscape\n",
    "5. Determine accuracy of model/function on the so far unused test-set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
